{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Install Required Libraries"
      ],
      "metadata": {
        "id": "cIBLDFKPcL6S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGefLAtxcGD3"
      },
      "outputs": [],
      "source": [
        "!pip install torch==2.3.1\n",
        "!pip install --user trl==0.11.4\n",
        "!pip install peft==0.14.0\n",
        "!pip install matplotlib==3.9.0\n",
        "!pip install pandas\n",
        "!pip install numpy==1.26.0\n",
        "!pip install --user datasets==3.2.0\n",
        "!pip install transformers==4.45.2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Import Libraries"
      ],
      "metadata": {
        "id": "WuQMutEwcTm4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import multiprocessing\n",
        "import os\n",
        "import requests\n",
        "import tarfile\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    GPT2Tokenizer,\n",
        "    set_seed,\n",
        "    GenerationConfig,\n",
        ")\n",
        "from trl import DPOConfig, DPOTrainer"
      ],
      "metadata": {
        "id": "Y5wt43WUcGth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Load and Configure Model & Tokenizer"
      ],
      "metadata": {
        "id": "lrxW4WenccKI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "model_ref = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "model.config.use_cache = False\n",
        "\n",
        "model"
      ],
      "metadata": {
        "id": "poEaN4HzcGyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Load and Preprocess Dataset"
      ],
      "metadata": {
        "id": "Yoj-5ebmcgaY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load dataset\n",
        "ds = load_dataset(\"BarraHome/ultrafeedback_binarized\")\n",
        "\n",
        "#Reduce dataset size\n",
        "for key in ds:\n",
        "    cnt = 50\n",
        "    ds[key] = ds[key].select(range(cnt))\n",
        "\n",
        "#Process dataset\n",
        "def process(row):\n",
        "    del row[\"prompt_id\"], row[\"messages\"], row[\"score_chosen\"], row[\"score_rejected\"]\n",
        "    row[\"chosen\"] = row[\"chosen\"][-1][\"content\"]\n",
        "    row[\"rejected\"] = row[\"rejected\"][-1][\"content\"]\n",
        "    return row\n",
        "\n",
        "ds = ds.map(process, num_proc=multiprocessing.cpu_count(), load_from_cache_file=False)\n",
        "\n",
        "train_dataset = ds['train_prefs']\n",
        "eval_dataset = ds['test_prefs']\n",
        "\n",
        "train_dataset[0]"
      ],
      "metadata": {
        "id": "OkHymKuqcG08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##LoRA Configuration"
      ],
      "metadata": {
        "id": "0vA5cgj-clzH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "peft_config = LoraConfig(\n",
        "    r=4,\n",
        "    target_modules=['c_proj', 'c_attn'],\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    lora_alpha=8,\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        ")"
      ],
      "metadata": {
        "id": "JlJI4fSEcG3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##DPO Configuration"
      ],
      "metadata": {
        "id": "Ad66yTcpcpI3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = DPOConfig(\n",
        "    beta=0.1,\n",
        "    output_dir=\"dpo\",\n",
        "    num_train_epochs=5,\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    remove_unused_columns=False,\n",
        "    logging_steps=10,\n",
        "    gradient_accumulation_steps=1,\n",
        "    learning_rate=1e-4,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    warmup_steps=2,\n",
        "    fp16=False,\n",
        "    save_steps=500,\n",
        "    report_to='none'\n",
        ")"
      ],
      "metadata": {
        "id": "66zGN6ydcG5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##DPO Trainer Initialization"
      ],
      "metadata": {
        "id": "uDcN2iINct_H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = DPOTrainer(\n",
        "    model=model,\n",
        "    ref_model=None,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    peft_config=peft_config,\n",
        "    max_length=512,\n",
        ")"
      ],
      "metadata": {
        "id": "Z1irMC8-cG75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model Training"
      ],
      "metadata": {
        "id": "OxC43dLocyFX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "SE8wV_3XcG-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Plot Training vs Evaluation Loss"
      ],
      "metadata": {
        "id": "Ti4rIYtGc4wA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "log = pd.DataFrame(trainer.state.log_history)\n",
        "log_t = log[log['loss'].notna()]\n",
        "log_e = log[log['eval_loss'].notna()]\n",
        "\n",
        "plt.plot(log_t[\"epoch\"], log_t[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(log_e[\"epoch\"], log_e[\"eval_loss\"], label=\"eval_loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3zuvOXl_cHAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Load Trained or Pre-Trained DPO Model"
      ],
      "metadata": {
        "id": "k3seR6J2c-He"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Try loading trained model\n",
        "try:\n",
        "    dpo_model = AutoModelForCausalLM.from_pretrained('./dpo/checkpoint-250')\n",
        "except:\n",
        "    #If unavailable, load pre-trained checkpoint\n",
        "    url = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/YIDeT3qihEpWChdXN_RmTg/DPO-tar.gz'\n",
        "    filename = './DPO.tar'\n",
        "\n",
        "    response = requests.get(url)\n",
        "    with open(filename, 'wb') as f:\n",
        "        f.write(response.content)\n",
        "\n",
        "    if tarfile.is_tarfile(filename):\n",
        "        with tarfile.open(filename, 'r') as tar:\n",
        "            tar.extractall()\n",
        "            print(\"Files extracted:\", tar.getnames())\n",
        "    dpo_model = AutoModelForCausalLM.from_pretrained('./DPO')"
      ],
      "metadata": {
        "id": "6ejkx6FAcHF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Generate and Compare Responses"
      ],
      "metadata": {
        "id": "5eWsOgFRdC4t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(42)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "generation_config = GenerationConfig(\n",
        "    do_sample=True,\n",
        "    top_k=1,\n",
        "    temperature=0.1,\n",
        "    max_new_tokens=25,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "PROMPT = \"Is a higher octane gasoline better for your car?\"\n",
        "inputs = tokenizer(PROMPT, return_tensors='pt')\n",
        "\n",
        "#DPO model response\n",
        "outputs_dpo = dpo_model.generate(**inputs, generation_config=generation_config)\n",
        "print(\"DPO response:\\t\", tokenizer.decode(outputs_dpo[0], skip_special_tokens=True))\n",
        "\n",
        "#GPT-2 baseline response\n",
        "gpt2_model = AutoModelForCausalLM.from_pretrained('gpt2')\n",
        "outputs_gpt2 = gpt2_model.generate(**inputs, generation_config=generation_config)\n",
        "print(\"\\nGPT2 response:\\t\", tokenizer.decode(outputs_gpt2[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "id": "rT90Rvv6cHIq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}